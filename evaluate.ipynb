{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e04f2b6-759e-441c-a22b-f1ee8b522710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2+cu121)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: rouge in /opt/conda/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (0.42.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers datasets nltk rouge jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81c33453-2fba-4219-8d7e-f4015e83738b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a95787be-f243-43fc-b5ac-280492f8f7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # 字符级别\n",
    "    # decoded_preds = [\" \".join((pred.replace(\" \", \"\"))) for pred in predictions]\n",
    "    # decoded_labels = [\" \".join((label.replace(\" \", \"\"))) for label in labels]\n",
    "\n",
    "    # 词级别\n",
    "    decoded_preds = [\" \".join(jieba.cut(pred.replace(\" \", \"\"))) for pred in predictions]\n",
    "    decoded_labels = [\" \".join(jieba.cut(label.replace(\" \", \"\"))) for label in labels]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    rouge = Rouge()\n",
    "\n",
    "    bleu =np.array([0.,0.,0.,0.])\n",
    "    weights = [(1.,0.,0.,0.),(1./2., 1./2.),(1./3., 1./3., 1./3.),(1./4., 1./4., 1./4., 1./4.)]\n",
    "    for decoded_label, decoded_pred in zip(decoded_labels, decoded_preds):\n",
    "        bleu +=np.array( sentence_bleu(\n",
    "            references=[decoded_label.split(' ')],\n",
    "            hypothesis=decoded_pred.split(' '),\n",
    "            smoothing_function=SmoothingFunction().method1,weights=weights\n",
    "        ))\n",
    "    bleu /= len(decoded_labels)\n",
    "    result = rouge.get_scores(decoded_preds, decoded_labels, avg=True)\n",
    "    result = {key: value['f'] * 100 for key, value in result.items()}\n",
    "    result[\"bleu\"] = {'bleu_1':bleu[0] * 100,'bleu_2':bleu[1] * 100,'bleu_3':bleu[2] * 100,'bleu_4':bleu[3] * 100}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7557c7ad-56ed-4945-bd16-b3cf8ff01e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 加载模型\n",
    "def load_model(model_name_or_path, load_in_4bit=False, adapter_name_or_path=None):\n",
    "    if load_in_4bit:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "        )\n",
    "    else:\n",
    "        quantization_config = None\n",
    "\n",
    "    # 加载base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "    # 加载adapter\n",
    "    if adapter_name_or_path is not None:\n",
    "        model = PeftModel.from_pretrained(model, adapter_name_or_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "## 加载tokenzier\n",
    "def load_tokenizer(model_name_or_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        use_fast=False\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "## 构建prompt\n",
    "def build_prompt(tokenizer, template, query, history, system=None):\n",
    "    template_name = template.template_name\n",
    "    system_format = template.system_format\n",
    "    user_format = template.user_format\n",
    "    assistant_format = template.assistant_format\n",
    "    system = system if system is not None else template.system\n",
    "\n",
    "    history.append({\"role\": 'user', 'message': query})\n",
    "    input_ids = []\n",
    "\n",
    "    # 添加系统信息\n",
    "    if system_format is not None:\n",
    "        if system is not None:\n",
    "            system_text = system_format.format(content=system)\n",
    "            input_ids = tokenizer.encode(system_text, add_special_tokens=False)\n",
    "    # 拼接历史对话\n",
    "    for item in history:\n",
    "        role, message = item['role'], item['message']\n",
    "        if role == 'user':\n",
    "            message = user_format.format(content=message, stop_token=tokenizer.eos_token)\n",
    "        else:\n",
    "            message = assistant_format.format(content=message, stop_token=tokenizer.eos_token)\n",
    "        tokens = tokenizer.encode(message, add_special_tokens=False)\n",
    "        input_ids += tokens\n",
    "    input_ids = torch.tensor([input_ids], dtype=torch.long)\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e41606d-473e-4324-83a4-62e8dc187c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ba0b55b-ea50-489e-891f-33c78a634508",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 16:56:58,222 - modelscope - INFO - Model revision not specified, use default: master in development mode\n",
      "2024-05-06 16:56:58,223 - modelscope - INFO - Development mode use revision: master\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:24<00:00, 21.15s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:92542 for open-end generation.\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.754 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.754 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG:jieba:Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': 28.01932317160261, 'rouge-2': 5.607476143671009, 'rouge-l': 17.391303847931123, 'bleu': {'bleu_1': 28.125, 'bleu_2': 11.778505162222508, 'bleu_3': 6.785769880043792, 'bleu_4': 1.9391047819483649}}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,DataCollatorWithPadding\n",
    "from qwen_generation_utils import  decode_tokens\n",
    "import torch\n",
    "import datasets\n",
    "from modelscope import snapshot_download\n",
    "\n",
    "\n",
    "model_name_or_path = snapshot_download('LLM-Research/Meta-Llama-3-8B-Instruct', \n",
    "                                           cache_dir='LLM-Research/Meta-Llama-3-8B-Instruct')#'./model'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, device_map=\"auto\", padding_side='left',trust_remote_code=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# # Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and might cause OOM Error.\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\",pad_token_id=tokenizer.eos_token_id, trust_remote_code=True, torch_dtype=torch.float16)\n",
    "# # (Optional) If on low resource devices, you can load model in 4-bit or 8-bit to further save GPU memory via bitsandbytes.\n",
    "#   # InternLM 7B in 4bit will cost nearly 8GB GPU memory.\n",
    "#   # pip install -U bitsandbytes\n",
    "#   # 8-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, load_in_8bit=True)\n",
    "#   # 4-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, load_in_4bit=True)\n",
    "# model = model.eval()\n",
    "# print_user = True # 控制是否输入提示输入框，用于notebook时，改为True\n",
    "\n",
    "# template_name = 'llama3'\n",
    "adapter_name_or_path = None\n",
    "\n",
    "# # template = template_dict[template_name]    \n",
    "\n",
    "# 若开启4bit推理能够节省很多显存，但效果可能下降\n",
    "load_in_4bit = False\n",
    "\n",
    "# 生成超参配置，可修改以取得更好的效果\n",
    "max_new_tokens = 500 # 每次回复时，AI生成文本的最大长度\n",
    "top_p = 0.9\n",
    "temperature = 0.6 # 越大越有创造性，越小越保守\n",
    "repetition_penalty = 1.1 # 越大越能避免吐字重复\n",
    "\n",
    "\n",
    "model = load_model(\n",
    "        model_name_or_path,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        adapter_name_or_path=adapter_name_or_path\n",
    "    ).eval()\n",
    "tokenizer = load_tokenizer(model_name_or_path if adapter_name_or_path is None else adapter_name_or_path)\n",
    "\n",
    "for para in model.parameters():\n",
    "    para.data =  para.data.to(torch.float32)\n",
    "# if template.stop_word is None:\n",
    "#     template.stop_word = tokenizer.eos_token\n",
    "# stop_token_id = tokenizer.encode(template.stop_word, add_special_tokens=True)\n",
    "# # assert len(stop_token_id) == 1\n",
    "# stop_token_id = stop_token_id[0]\n",
    "\n",
    "\n",
    "# # convert data\n",
    "# import ujson\n",
    "# def transform_conversation_data(raw_data):\n",
    "#     try:\n",
    "#         instruction = '<|im_start|>system\\n'+raw_data.get(\"conversation\", \"\")[0]['system'] + \"<|im_end|>\\n\"\n",
    "\n",
    "#         conversation = raw_data.get(\"conversation\", [])\n",
    "#         for i, dialog in enumerate(conversation):\n",
    "#             instruction += \"<|im_start|>user\\n来访者：\" + dialog[\"input\"]+ \"<|im_end|>\\n\"\n",
    "\n",
    "#             if i < len(conversation) - 1:\n",
    "#                 instruction += \"<|im_start|>assistant\\n医生：\" + dialog[\"output\"]+\"<|im_end|>\\n\"\n",
    "\n",
    "#         response = conversation[-1][\"output\"] if conversation else \"\"\n",
    "\n",
    "#         instruction +=\"<|im_start|>assistant\\n医生：\"\n",
    "\n",
    "#         return {\"instruction\": instruction, \"output\": response}\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         pass\n",
    "\n",
    "\n",
    "# with open(f'./data_dir/data.json', 'r', encoding='utf-8') as f1:\n",
    "#     data = ujson.load(f1)\n",
    "# with open(f'./data_dir/converted.json', 'w', encoding='utf-8') as f:\n",
    "#     for j, item in enumerate(data):\n",
    "#         temp=transform_conversation_data(item)\n",
    "#         if temp:\n",
    "#             transformed_data =ujson.dumps(temp, ensure_ascii=False)\n",
    "#             f.write(transformed_data+'\\n')\n",
    "\n",
    "#set test params\n",
    "\n",
    "\n",
    "#set test params\n",
    "test_num=1590 #测试数据条数\n",
    "batch_size=12\n",
    "\n",
    "\n",
    "#prepare data and dataloader\n",
    "dataset = datasets.load_dataset('json', data_files='converted.json',split=f\"train[:{test_num}]\")\n",
    "references =dataset['output'][:test_num]\n",
    "\n",
    "hypotheses = []\n",
    "def preprocess(data):\n",
    "    length = list(map(len, data['instruction']))\n",
    "    model_inputs=tokenizer(data['instruction'], max_length=512, truncation=True )\n",
    "    labels=tokenizer(data['output'], padding=True,max_length=128, truncation=True )\n",
    "    model_inputs['labels']=labels['input_ids']\n",
    "    model_inputs['length'] = length\n",
    "    return model_inputs\n",
    "preprocessed_dataset = dataset.map(preprocess, batched=True,remove_columns=['instruction', 'output',])\n",
    "\n",
    "\n",
    "collator=DataCollatorWithPadding(tokenizer=tokenizer,)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(preprocessed_dataset, batch_size=batch_size, collate_fn=collator)\n",
    "\n",
    "#generate responses\n",
    "stop_word=\"<|im_end|>\"\n",
    "count = 0\n",
    "for batch in dataloader:\n",
    "    count += 1\n",
    "    batch_input_ids = torch.LongTensor(batch['input_ids']).to(model.device)\n",
    "    batch_labels = batch['labels']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    length = batch['length']\n",
    "    batch_out_ids = model.generate(\n",
    "        batch_input_ids.to(model.device),\n",
    "        return_dict_in_generate=False,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        eos_token_id=92542\n",
    "    )\n",
    "    \n",
    "    padding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]\n",
    "    batch_response = [\n",
    "    decode_tokens(\n",
    "        batch_out_ids[i][padding_lens[i]:],\n",
    "        tokenizer,\n",
    "        context_length=0,\n",
    "        raw_text_len=length[i],\n",
    "        chat_format=\"raw\",\n",
    "        verbose=False,\n",
    "        errors='replace'\n",
    "    ).replace(\"医生：\",\"\") for i in range(batch_size)]\n",
    "    hypotheses.extend([r.replace(stop_word,\" \").split()[0] if stop_word in r else r for r in batch_response])\n",
    "    print(count)\n",
    "\n",
    "\n",
    "# Load metric\n",
    "# from metric import compute_metrics\n",
    "\n",
    "print(compute_metrics((hypotheses,references)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7418e0f1-0b18-4fc8-a72c-cb27851abc94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
