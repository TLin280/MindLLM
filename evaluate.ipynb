{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e04f2b6-759e-441c-a22b-f1ee8b522710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:06:41.915177Z",
     "iopub.status.busy": "2024-04-30T16:06:41.914872Z",
     "iopub.status.idle": "2024-04-30T16:06:45.573434Z",
     "shell.execute_reply": "2024-04-30T16:06:45.572871Z",
     "shell.execute_reply.started": "2024-04-30T16:06:41.915158Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2+cu121)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: rouge in /opt/conda/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (0.42.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers datasets nltk rouge jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e35c02b-381e-4363-8b30-d677b2547165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:06:47.892015Z",
     "iopub.status.busy": "2024-04-30T16:06:47.891667Z",
     "iopub.status.idle": "2024-04-30T16:06:48.121156Z",
     "shell.execute_reply": "2024-04-30T16:06:48.120574Z",
     "shell.execute_reply.started": "2024-04-30T16:06:47.891996Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './train_dir/data.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./train_dir/data.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f1:\n\u001b[1;32m     24\u001b[0m     data \u001b[38;5;241m=\u001b[39m ujson\u001b[38;5;241m.\u001b[39mload(f1)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./train_dir/converted.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train_dir/data.json'"
     ]
    }
   ],
   "source": [
    "import ujson\n",
    "def transform_conversation_data(raw_data):\n",
    "    try:\n",
    "        instruction = raw_data.get(\"conversation\", \"\")[0]['system'] + \"\\n\\n对话：\"\n",
    "\n",
    "        conversation = raw_data.get(\"conversation\", [])\n",
    "        for i, dialog in enumerate(conversation):\n",
    "            instruction += \"\\n来访者：\" + dialog[\"input\"]\n",
    "\n",
    "            if i < len(conversation) - 1:\n",
    "                instruction += \"\\n医生：\" + dialog[\"output\"]\n",
    "\n",
    "        response = conversation[-1][\"output\"] if conversation else \"\"\n",
    "\n",
    "        instruction += \"\\n医生：\"\n",
    "\n",
    "        return {\"instruction\": instruction, \"output\": response}\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "with open(f'./train_dir/data.json', 'r', encoding='utf-8') as f1:\n",
    "    data = ujson.load(f1)\n",
    "with open(f'./train_dir/converted.json', 'w', encoding='utf-8') as f:\n",
    "    for j, item in enumerate(data):\n",
    "        temp=transform_conversation_data(item)\n",
    "        if temp:\n",
    "            transformed_data =ujson.dumps(temp, ensure_ascii=False)\n",
    "            f.write(transformed_data+'\\n')\n",
    "    print('********')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c33453-2fba-4219-8d7e-f4015e83738b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:29:22.715277Z",
     "iopub.status.busy": "2024-04-30T16:29:22.714895Z",
     "iopub.status.idle": "2024-04-30T16:29:23.039595Z",
     "shell.execute_reply": "2024-04-30T16:29:23.039007Z",
     "shell.execute_reply.started": "2024-04-30T16:29:22.715253Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a95787be-f243-43fc-b5ac-280492f8f7a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:29:24.485987Z",
     "iopub.status.busy": "2024-04-30T16:29:24.485629Z",
     "iopub.status.idle": "2024-04-30T16:29:24.491650Z",
     "shell.execute_reply": "2024-04-30T16:29:24.491119Z",
     "shell.execute_reply.started": "2024-04-30T16:29:24.485970Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # 字符级别\n",
    "    # decoded_preds = [\" \".join((pred.replace(\" \", \"\"))) for pred in predictions]\n",
    "    # decoded_labels = [\" \".join((label.replace(\" \", \"\"))) for label in labels]\n",
    "\n",
    "    # 词级别\n",
    "    decoded_preds = [\" \".join(jieba.cut(pred.replace(\" \", \"\"))) for pred in predictions]\n",
    "    decoded_labels = [\" \".join(jieba.cut(label.replace(\" \", \"\"))) for label in labels]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    rouge = Rouge()\n",
    "\n",
    "    bleu =np.array([0.,0.,0.,0.])\n",
    "    weights = [(1.,0.,0.,0.),(1./2., 1./2.),(1./3., 1./3., 1./3.),(1./4., 1./4., 1./4., 1./4.)]\n",
    "    for decoded_label, decoded_pred in zip(decoded_labels, decoded_preds):\n",
    "        bleu +=np.array( sentence_bleu(\n",
    "            references=[decoded_label.split(' ')],\n",
    "            hypothesis=decoded_pred.split(' '),\n",
    "            smoothing_function=SmoothingFunction().method1,weights=weights\n",
    "        ))\n",
    "    bleu /= len(decoded_labels)\n",
    "    result = rouge.get_scores(decoded_preds, decoded_labels, avg=True)\n",
    "    result = {key: value['f'] * 100 for key, value in result.items()}\n",
    "    result[\"bleu\"] = {'bleu_1':bleu[0] * 100,'bleu_2':bleu[1] * 100,'bleu_3':bleu[2] * 100,'bleu_4':bleu[3] * 100}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7557c7ad-56ed-4945-bd16-b3cf8ff01e61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:32:42.640671Z",
     "iopub.status.busy": "2024-04-30T16:32:42.640367Z",
     "iopub.status.idle": "2024-04-30T16:32:42.647189Z",
     "shell.execute_reply": "2024-04-30T16:32:42.646669Z",
     "shell.execute_reply.started": "2024-04-30T16:32:42.640653Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 加载模型\n",
    "def load_model(model_name_or_path, load_in_4bit=False, adapter_name_or_path=None):\n",
    "    if load_in_4bit:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "        )\n",
    "    else:\n",
    "        quantization_config = None\n",
    "\n",
    "    # 加载base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "    # 加载adapter\n",
    "    if adapter_name_or_path is not None:\n",
    "        model = PeftModel.from_pretrained(model, adapter_name_or_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "## 加载tokenzier\n",
    "def load_tokenizer(model_name_or_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        use_fast=False\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "## 构建prompt\n",
    "def build_prompt(tokenizer, template, query, history, system=None):\n",
    "    template_name = template.template_name\n",
    "    system_format = template.system_format\n",
    "    user_format = template.user_format\n",
    "    assistant_format = template.assistant_format\n",
    "    system = system if system is not None else template.system\n",
    "\n",
    "    history.append({\"role\": 'user', 'message': query})\n",
    "    input_ids = []\n",
    "\n",
    "    # 添加系统信息\n",
    "    if system_format is not None:\n",
    "        if system is not None:\n",
    "            system_text = system_format.format(content=system)\n",
    "            input_ids = tokenizer.encode(system_text, add_special_tokens=False)\n",
    "    # 拼接历史对话\n",
    "    for item in history:\n",
    "        role, message = item['role'], item['message']\n",
    "        if role == 'user':\n",
    "            message = user_format.format(content=message, stop_token=tokenizer.eos_token)\n",
    "        else:\n",
    "            message = assistant_format.format(content=message, stop_token=tokenizer.eos_token)\n",
    "        tokens = tokenizer.encode(message, add_special_tokens=False)\n",
    "        input_ids += tokens\n",
    "    input_ids = torch.tensor([input_ids], dtype=torch.long)\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ba0b55b-ea50-489e-891f-33c78a634508",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-30T16:35:39.609347Z",
     "iopub.status.busy": "2024-04-30T16:35:39.608981Z",
     "iopub.status.idle": "2024-04-30T16:35:40.552863Z",
     "shell.execute_reply": "2024-04-30T16:35:40.552176Z",
     "shell.execute_reply.started": "2024-04-30T16:35:39.609327Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.6\u001b[39m \u001b[38;5;66;03m# 越大越有创造性，越小越保守\u001b[39;00m\n\u001b[1;32m     34\u001b[0m repetition_penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.1\u001b[39m \u001b[38;5;66;03m# 越大越能避免吐字重复\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name_or_path\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     42\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load_tokenizer(model_name_or_path \u001b[38;5;28;01mif\u001b[39;00m adapter_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m adapter_name_or_path)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m template\u001b[38;5;241m.\u001b[39mstop_word \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_name_or_path, load_in_4bit, adapter_name_or_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m     quantization_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 加载base model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 加载adapter\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m adapter_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3434\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3429\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   3430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has some weights that should be kept in higher precision, you need to upgrade \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`accelerate` to properly deal with them (`pip install --upgrade accelerate`).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3432\u001b[0m     )\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequential\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 3434\u001b[0m     max_memory \u001b[38;5;241m=\u001b[39m \u001b[43mget_balanced_memory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_zero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalanced_low_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3439\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdevice_map_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3440\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3442\u001b[0m     max_memory \u001b[38;5;241m=\u001b[39m get_max_memory(max_memory)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py:849\u001b[0m, in \u001b[0;36mget_balanced_memory\u001b[0;34m(model, max_memory, no_split_module_classes, dtype, special_dtypes, low_zero)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;66;03m# Get default / clean up max_memory\u001b[39;00m\n\u001b[1;32m    848\u001b[0m user_not_set_max_memory \u001b[38;5;241m=\u001b[39m max_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 849\u001b[0m max_memory \u001b[38;5;241m=\u001b[39m \u001b[43mget_max_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_npu_available():\n\u001b[1;32m    852\u001b[0m     num_devices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m([d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m max_memory \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(d)\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m max_memory[d] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py:720\u001b[0m, in \u001b[0;36mget_max_memory\u001b[0;34m(max_memory)\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()):\n\u001b[0;32m--> 720\u001b[0m             _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m         max_memory \u001b[38;5;241m=\u001b[39m {i: torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmem_get_info(i)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())}\n\u001b[1;32m    722\u001b[0m \u001b[38;5;66;03m# allocate everything in the mps device as the RAM is shared\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,DataCollatorWithPadding\n",
    "from qwen_generation_utils import  decode_tokens\n",
    "import torch\n",
    "import datasets\n",
    "from modelscope import snapshot_download\n",
    "\n",
    "\n",
    "model_name_or_path = snapshot_download('LLM-Research/Meta-Llama-3-8B-Instruct', \n",
    "                                           cache_dir='LLM-Research/Meta-Llama-3-8B-Instruct')#'./model'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_dir, device_map=\"auto\", padding_side='left',trust_remote_code=True)\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# # Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and might cause OOM Error.\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\",pad_token_id=tokenizer.eos_token_id, trust_remote_code=True, torch_dtype=torch.float16)\n",
    "# # (Optional) If on low resource devices, you can load model in 4-bit or 8-bit to further save GPU memory via bitsandbytes.\n",
    "#   # InternLM 7B in 4bit will cost nearly 8GB GPU memory.\n",
    "#   # pip install -U bitsandbytes\n",
    "#   # 8-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, load_in_8bit=True)\n",
    "#   # 4-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, load_in_4bit=True)\n",
    "# model = model.eval()\n",
    "print_user = True # 控制是否输入提示输入框，用于notebook时，改为True\n",
    "\n",
    "template_name = 'llama3'\n",
    "adapter_name_or_path = None\n",
    "\n",
    "# template = template_dict[template_name]    \n",
    "\n",
    "# 若开启4bit推理能够节省很多显存，但效果可能下降\n",
    "load_in_4bit = False\n",
    "\n",
    "# 生成超参配置，可修改以取得更好的效果\n",
    "max_new_tokens = 500 # 每次回复时，AI生成文本的最大长度\n",
    "top_p = 0.9\n",
    "temperature = 0.6 # 越大越有创造性，越小越保守\n",
    "repetition_penalty = 1.1 # 越大越能避免吐字重复\n",
    "\n",
    "\n",
    "model = load_model(\n",
    "        model_name_or_path,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        adapter_name_or_path=adapter_name_or_path\n",
    "    ).eval()\n",
    "tokenizer = load_tokenizer(model_name_or_path if adapter_name_or_path is None else adapter_name_or_path)\n",
    "if template.stop_word is None:\n",
    "    template.stop_word = tokenizer.eos_token\n",
    "stop_token_id = tokenizer.encode(template.stop_word, add_special_tokens=True)\n",
    "# assert len(stop_token_id) == 1\n",
    "stop_token_id = stop_token_id[0]\n",
    "\n",
    "\n",
    "# # convert data\n",
    "# import ujson\n",
    "# def transform_conversation_data(raw_data):\n",
    "#     try:\n",
    "#         instruction = '<|im_start|>system\\n'+raw_data.get(\"conversation\", \"\")[0]['system'] + \"<|im_end|>\\n\"\n",
    "\n",
    "#         conversation = raw_data.get(\"conversation\", [])\n",
    "#         for i, dialog in enumerate(conversation):\n",
    "#             instruction += \"<|im_start|>user\\n来访者：\" + dialog[\"input\"]+ \"<|im_end|>\\n\"\n",
    "\n",
    "#             if i < len(conversation) - 1:\n",
    "#                 instruction += \"<|im_start|>assistant\\n医生：\" + dialog[\"output\"]+\"<|im_end|>\\n\"\n",
    "\n",
    "#         response = conversation[-1][\"output\"] if conversation else \"\"\n",
    "\n",
    "#         instruction +=\"<|im_start|>assistant\\n医生：\"\n",
    "\n",
    "#         return {\"instruction\": instruction, \"output\": response}\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         pass\n",
    "\n",
    "\n",
    "# with open(f'./data_dir/data.json', 'r', encoding='utf-8') as f1:\n",
    "#     data = ujson.load(f1)\n",
    "# with open(f'./data_dir/converted.json', 'w', encoding='utf-8') as f:\n",
    "#     for j, item in enumerate(data):\n",
    "#         temp=transform_conversation_data(item)\n",
    "#         if temp:\n",
    "#             transformed_data =ujson.dumps(temp, ensure_ascii=False)\n",
    "#             f.write(transformed_data+'\\n')\n",
    "\n",
    "#set test params\n",
    "\n",
    "\n",
    "#set test params\n",
    "test_num=1596 #测试数据条数\n",
    "batch_size=12\n",
    "\n",
    "\n",
    "#prepare data and dataloader\n",
    "dataset = datasets.load_dataset('json', data_files='converted.json',split=f\"train[:{test_num}]\")\n",
    "references =dataset['output'][:test_num]\n",
    "\n",
    "hypotheses = []\n",
    "def preprocess(data):\n",
    "    length = list(map(len, data['instruction']))\n",
    "    model_inputs=tokenizer(data['instruction'], max_length=512, truncation=True )\n",
    "    labels=tokenizer(data['output'], padding=True,max_length=128, truncation=True )\n",
    "    model_inputs['labels']=labels['input_ids']\n",
    "    model_inputs['length'] = length\n",
    "    return model_inputs\n",
    "preprocessed_dataset = dataset.map(preprocess, batched=True,remove_columns=['instruction', 'output',])\n",
    "\n",
    "\n",
    "collator=DataCollatorWithPadding(tokenizer=tokenizer,)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(preprocessed_dataset, batch_size=batch_size, collate_fn=collator)\n",
    "\n",
    "#generate responses\n",
    "stop_word=\"<|im_end|>\"\n",
    "for batch in dataloader:\n",
    "    batch_input_ids = torch.LongTensor(batch['input_ids']).to(model.device)\n",
    "    batch_labels = batch['labels']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    length = batch['length']\n",
    "    batch_out_ids = model.generate(\n",
    "        batch_input_ids.to(model.device),\n",
    "        return_dict_in_generate=False,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        eos_token_id=92542\n",
    "    )\n",
    "    \n",
    "    padding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]\n",
    "    batch_response = [\n",
    "    decode_tokens(\n",
    "        batch_out_ids[i][padding_lens[i]:],\n",
    "        tokenizer,\n",
    "        context_length=0,\n",
    "        raw_text_len=length[i],\n",
    "        chat_format=\"raw\",\n",
    "        verbose=False,\n",
    "        errors='replace'\n",
    "    ).replace(\"医生：\",\"\") for i in range(batch_size)]\n",
    "    hypotheses.extend([r.replace(stop_word,\" \").split()[0] if stop_word in r else r for r in batch_response])\n",
    "\n",
    "\n",
    "# Load metric\n",
    "from metric import compute_metrics\n",
    "\n",
    "print(compute_metrics((hypotheses,references)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
